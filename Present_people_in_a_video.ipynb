{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Present people in a video.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw1oT3vwayru"
      },
      "source": [
        "# This code use face recognition to detect and store a list of present people from a video\n",
        "\n",
        "\n",
        "You need OpenCV and face_recognition installed to run this example. To install it, runðŸ‡°\n",
        " ___pip install opencv-python___  \n",
        "___pip install face_recognition___\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFQ388uFa5Xn",
        "outputId": "94524f32-71cc-41e9-8e26-2f1e4db5fd2b"
      },
      "source": [
        "!pip install face_recognition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: face_recognition in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZhaPZ-nayr3",
        "outputId": "d81a37c3-7acc-4c1c-f896-f401e966c006"
      },
      "source": [
        "%pylab inline \n",
        "import face_recognition\n",
        "import cv2\n",
        "import matplotlib.patches as patches\n",
        "from IPython.display import clear_output\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vFFMtbGqhV8",
        "outputId": "b8305cf7-60d1-49d6-e10d-05be262b7228"
      },
      "source": [
        "cd ../pictures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pictures\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTfv5QUKucBd"
      },
      "source": [
        "We will Create a *person_image* variable and a *person_face_encoding* variable for each student picture in the folder pictures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-czz1vig3zj"
      },
      "source": [
        "import os\n",
        "\n",
        "path = \"../pictures\"\n",
        "\n",
        "folder = os.fsencode(path)\n",
        "\n",
        "filenames = []\n",
        "i = 1\n",
        "\n",
        "for file in os.listdir(folder):\n",
        "    filename = os.fsdecode(file)\n",
        "    if filename.endswith( ('.jpeg', '.png', '.gif', 'png') ):\n",
        "      globals()[\"person_image\" + str(i)] = face_recognition.load_image_file(filename)\n",
        "      globals()[\"person_face_encoding\" + str(i)] = face_recognition.face_encodings(globals()[\"person_image\" + str(i)])[0]\n",
        "      i += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFOHb6bls_W1",
        "outputId": "b8cc88d2-dea5-4598-9ab6-85b0ff0093ef"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2qNoZSYuvJ5"
      },
      "source": [
        "___ \n",
        "**NEXT** , using the video stored as Movie.mp4, and the created arrays of known face encodings and their names.\n",
        "\n",
        " we will append the names of present people in a list. \n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y75y_Ujayr7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "91e328ab-182c-41f2-9c81-d6038aed590a"
      },
      "source": [
        "video_capture = cv2.VideoCapture(\"./Movie.mp4\")\n",
        "\n",
        "\n",
        "# Create arrays of known face encodings and their names\n",
        "known_face_encodings = [\n",
        "    person_face_encoding1,\n",
        "    person_face_encoding2\n",
        "]\n",
        "known_face_names = [\n",
        "    \"Person1\",\n",
        "    \"Person2\"\n",
        "]\n",
        "\n",
        "# Initialize some variables\n",
        "face_locations = []\n",
        "face_encodings = []\n",
        "face_names = []\n",
        "    \n",
        "present = []\n",
        "frame_count = 0\n",
        "\n",
        "while video_capture.isOpened():    \n",
        "    # Grab a single frame of video\n",
        "    ret, frame = video_capture.read()\n",
        "\n",
        "    # Bail out when the video file ends\n",
        "    if not ret:\n",
        "        video_capture.release()\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "    if frame_count % 15 == 0:    \n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "       # Display video frame\n",
        "        title(\"Input Stream\")\n",
        "        plt.imshow(frame)   \n",
        "        # Find all the faces and face encodings in the current frame of video\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "        face_locations = face_recognition.face_locations(rgb_frame)\n",
        "        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
        "\n",
        "        face_names = []\n",
        "        for face_encoding in face_encodings:\n",
        "            # See if the face is a match for the known face(s)\n",
        "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
        "            name = \"Unknown\"\n",
        "\n",
        "\n",
        "            # # If a match was found in known_face_encodings, just use the first one.\n",
        "            # if True in matches:\n",
        "            #     first_match_index = matches.index(True)\n",
        "            #     name = known_face_names[first_match_index]\n",
        "\n",
        "            # Or instead, use the known face with the smallest distance to the new face\n",
        "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
        "            best_match_index = np.argmin(face_distances)\n",
        "            if matches[best_match_index]:\n",
        "                name = known_face_names[best_match_index]\n",
        "            \n",
        "            \n",
        "            present.append(name)\n",
        "\n",
        "        # If faces were found, we will mark it on frame with blue dots\n",
        "        for face_location in face_locations:        \n",
        "            plt.plot(face_location[1], face_location[0], 'bo')\n",
        "            plt.plot(face_location[1], face_location[2], 'bo')\n",
        "            plt.plot(face_location[3], face_location[2], 'bo')\n",
        "            plt.plot(face_location[3], face_location[0], 'bo')\n",
        "      \n",
        "        # Show frame...\n",
        "        plt.show() \n",
        "        # ... and hold it until a new frame appears\n",
        "        clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The present persons are :  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Person2', 'Person1', 'Unknown']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ee4Ul3fvEgU"
      },
      "source": [
        "___ \n",
        "**Now we can share the list of the present persons in the video.**\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puY_ss3tvDkT"
      },
      "source": [
        "present = list(set(present))\n",
        "print(\" The present persons are :  \")\n",
        "present"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}